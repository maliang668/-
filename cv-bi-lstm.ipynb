{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import copy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from keras.layers import GlobalAveragePooling1D, concatenate, CuDNNLSTM\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_pre2.csv')\n",
    "test = pd.read_csv('test_pre2.csv')\n",
    "subm = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cv_id'] = [random.randint(1,10) for _ in range(len(train))]\n",
    "test['cv_id'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train_list = train[list_classes].values\n",
    "X_test_list = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 150\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train_list) + list(X_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_list)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences_pad = sequence.pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "X_test_sequences_pad = sequence.pad_sequences(X_test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vec = 'crawl-300d-2M.vec'\n",
    "embedding_glove = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(embedding_vec, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_fast_text = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = model[word] if word in model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fast_text[i] = embedding_vector\n",
    "embedding_matrix_fast_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_glove = {}\n",
    "with open(embedding_glove,encoding ='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        cof = np.asarray(values[1:],dtype = 'float32')\n",
    "        embedding_index_glove[word] = cof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = np.zeros((nb_words,embed_size))\n",
    "for word,i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vec_glove = embedding_index_glove.get(word)\n",
    "    if embedding_vec_glove is not None:\n",
    "        embedding_matrix_glove[i] = embedding_vec_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x1 = Embedding(max_features,embed_size,weights=[embedding_matrix_glove])(inp)\n",
    "    x1 = SpatialDropout1D(0.1)(x1)\n",
    "    x1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x1)\n",
    "    \n",
    "    \n",
    "    x = Embedding(max_features,embed_size,weights=[embedding_matrix_fast_text])(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    \n",
    "    concat = concatenate([x1,x])\n",
    "    avg_pool = GlobalAveragePooling1D()(concat)\n",
    "    max_pool = GlobalMaxPooling1D()(concat)\n",
    "    concat_pool = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    \n",
    "    x = Dense(32, activation=\"relu\")(concat_pool)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(6, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models=[]\n",
    "cv_results=[]\n",
    "cv_scores=[]\n",
    "Kfold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "train_shape\n",
      "(143341, 150) (143341, 6)\n",
      "val_shape\n",
      "(16230, 150) (16230, 6)\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 183s 1ms/step - loss: 0.0800 - acc: 0.9719\n",
      "1 0 0.9800507026381684\n",
      "epoch 0 improved from -1 to 0.9800507026381684\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 180s 1ms/step - loss: 0.0432 - acc: 0.9837\n",
      "1 1 0.9843001310513645\n",
      "epoch 1 improved from 0.9800507026381684 to 0.9843001310513645\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 180s 1ms/step - loss: 0.0366 - acc: 0.9858\n",
      "1 2 0.9842615258589947\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 180s 1ms/step - loss: 0.0307 - acc: 0.9879\n",
      "1 3 0.9838517260689285\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 180s 1ms/step - loss: 0.0255 - acc: 0.9900\n",
      "1 4 0.9832659035889563\n",
      "Epoch 1/1\n",
      "143341/143341 [==============================] - 180s 1ms/step - loss: 0.0210 - acc: 0.9918\n",
      "1 5 0.9814830798272206\n",
      "fold 2\n",
      "train_shape\n",
      "(143479, 150) (143479, 6)\n",
      "val_shape\n",
      "(16092, 150) (16092, 6)\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 184s 1ms/step - loss: 0.0718 - acc: 0.9754\n",
      "2 0 0.9806708856331957\n",
      "epoch 0 improved from -1 to 0.9806708856331957\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0429 - acc: 0.9838\n",
      "2 1 0.9849125787914712\n",
      "epoch 1 improved from 0.9806708856331957 to 0.9849125787914712\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0374 - acc: 0.9853\n",
      "2 2 0.9861773692176916\n",
      "epoch 2 improved from 0.9849125787914712 to 0.9861773692176916\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0323 - acc: 0.9871\n",
      "2 3 0.9860001294990237\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0281 - acc: 0.9887\n",
      "2 4 0.9858581700978838\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0240 - acc: 0.9902\n",
      "2 5 0.9848965510777802\n",
      "Epoch 1/1\n",
      "143479/143479 [==============================] - 180s 1ms/step - loss: 0.0204 - acc: 0.9918\n",
      "2 6 0.985125797450729\n",
      "fold 3\n",
      "train_shape\n",
      "(143772, 150) (143772, 6)\n",
      "val_shape\n",
      "(15799, 150) (15799, 6)\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 184s 1ms/step - loss: 0.0716 - acc: 0.9763\n",
      "3 0 0.9799039655431621\n",
      "epoch 0 improved from -1 to 0.9799039655431621\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 180s 1ms/step - loss: 0.0407 - acc: 0.9843\n",
      "3 1 0.9863888724993432\n",
      "epoch 1 improved from 0.9799039655431621 to 0.9863888724993432\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 180s 1ms/step - loss: 0.0341 - acc: 0.9864\n",
      "3 2 0.985521945864261\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 180s 1ms/step - loss: 0.0289 - acc: 0.9884\n",
      "3 3 0.9851550532631994\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 180s 1ms/step - loss: 0.0234 - acc: 0.9906\n",
      "3 4 0.9838525377580516\n",
      "Epoch 1/1\n",
      "143772/143772 [==============================] - 180s 1ms/step - loss: 0.0188 - acc: 0.9927\n",
      "3 5 0.9817504355668302\n",
      "fold 4\n",
      "train_shape\n",
      "(143576, 150) (143576, 6)\n",
      "val_shape\n",
      "(15995, 150) (15995, 6)\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 184s 1ms/step - loss: 0.0670 - acc: 0.9779\n",
      "4 0 0.9828939184800269\n",
      "epoch 0 improved from -1 to 0.9828939184800269\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 180s 1ms/step - loss: 0.0416 - acc: 0.9836\n",
      "4 1 0.987653416089057\n",
      "epoch 1 improved from 0.9828939184800269 to 0.987653416089057\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 180s 1ms/step - loss: 0.0356 - acc: 0.9856\n",
      "4 2 0.9872774771419959\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 180s 1ms/step - loss: 0.0303 - acc: 0.9876\n",
      "4 3 0.986548943352135\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 180s 1ms/step - loss: 0.0256 - acc: 0.9896\n",
      "4 4 0.9849145569912877\n",
      "Epoch 1/1\n",
      "143576/143576 [==============================] - 180s 1ms/step - loss: 0.0216 - acc: 0.9913\n",
      "4 5 0.9830765669258251\n",
      "fold 5\n",
      "train_shape\n",
      "(143707, 150) (143707, 6)\n",
      "val_shape\n",
      "(15864, 150) (15864, 6)\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 184s 1ms/step - loss: 0.0604 - acc: 0.9800\n",
      "5 0 0.9878503102218236\n",
      "epoch 0 improved from -1 to 0.9878503102218236\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 180s 1ms/step - loss: 0.0395 - acc: 0.9846\n",
      "5 1 0.9896646228479963\n",
      "epoch 1 improved from 0.9878503102218236 to 0.9896646228479963\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 180s 1ms/step - loss: 0.0335 - acc: 0.9867\n",
      "5 2 0.9896023410788811\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 180s 1ms/step - loss: 0.0278 - acc: 0.9888\n",
      "5 3 0.9883589863870211\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 180s 1ms/step - loss: 0.0226 - acc: 0.9909\n",
      "5 4 0.9871886719641094\n",
      "Epoch 1/1\n",
      "143707/143707 [==============================] - 180s 1ms/step - loss: 0.0182 - acc: 0.9929\n",
      "5 5 0.98525155366864\n",
      "fold 6\n",
      "train_shape\n",
      "(143681, 150) (143681, 6)\n",
      "val_shape\n",
      "(15890, 150) (15890, 6)\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 184s 1ms/step - loss: 0.0680 - acc: 0.9782\n",
      "6 0 0.9793806876298508\n",
      "epoch 0 improved from -1 to 0.9793806876298508\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0422 - acc: 0.9836\n",
      "6 1 0.9838648807602665\n",
      "epoch 1 improved from 0.9793806876298508 to 0.9838648807602665\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0364 - acc: 0.9856\n",
      "6 2 0.9847420020607728\n",
      "epoch 2 improved from 0.9838648807602665 to 0.9847420020607728\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0312 - acc: 0.9872\n",
      "6 3 0.9836392420621008\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0268 - acc: 0.9891\n",
      "6 4 0.9831027597063988\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0228 - acc: 0.9908\n",
      "6 5 0.9824745880361618\n",
      "Epoch 1/1\n",
      "143681/143681 [==============================] - 180s 1ms/step - loss: 0.0196 - acc: 0.9921\n",
      "6 6 0.9814858475613272\n",
      "fold 7\n",
      "train_shape\n",
      "(143640, 150) (143640, 6)\n",
      "val_shape\n",
      "(15931, 150) (15931, 6)\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 184s 1ms/step - loss: 0.0655 - acc: 0.9780\n",
      "7 0 0.9848513388626047\n",
      "epoch 0 improved from -1 to 0.9848513388626047\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0403 - acc: 0.9843\n",
      "7 1 0.9881039030448474\n",
      "epoch 1 improved from 0.9848513388626047 to 0.9881039030448474\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0342 - acc: 0.9863\n",
      "7 2 0.9886027054089528\n",
      "epoch 2 improved from 0.9881039030448474 to 0.9886027054089528\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0288 - acc: 0.9882\n",
      "7 3 0.9876671718662232\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0241 - acc: 0.9904\n",
      "7 4 0.986773729752291\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0192 - acc: 0.9924\n",
      "7 5 0.9856314834854825\n",
      "Epoch 1/1\n",
      "143640/143640 [==============================] - 180s 1ms/step - loss: 0.0155 - acc: 0.9940\n",
      "7 6 0.9843108045289242\n",
      "fold 8\n",
      "train_shape\n",
      "(143577, 150) (143577, 6)\n",
      "val_shape\n",
      "(15994, 150) (15994, 6)\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 184s 1ms/step - loss: 0.0668 - acc: 0.9775\n",
      "8 0 0.9812032891691055\n",
      "epoch 0 improved from -1 to 0.9812032891691055\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0413 - acc: 0.9842\n",
      "8 1 0.9852186750206372\n",
      "epoch 1 improved from 0.9812032891691055 to 0.9852186750206372\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0349 - acc: 0.9862\n",
      "8 2 0.9864531247771567\n",
      "epoch 2 improved from 0.9852186750206372 to 0.9864531247771567\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0290 - acc: 0.9883\n",
      "8 3 0.985588472137119\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0240 - acc: 0.9904\n",
      "8 4 0.9836742573837478\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0196 - acc: 0.9923\n",
      "8 5 0.98263380103662\n",
      "Epoch 1/1\n",
      "143577/143577 [==============================] - 180s 1ms/step - loss: 0.0157 - acc: 0.9939\n",
      "8 6 0.9816187745660193\n",
      "fold 9\n",
      "train_shape\n",
      "(143718, 150) (143718, 6)\n",
      "val_shape\n",
      "(15853, 150) (15853, 6)\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 184s 1ms/step - loss: 0.0633 - acc: 0.9791\n",
      "9 0 0.9851643766644852\n",
      "epoch 0 improved from -1 to 0.9851643766644852\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 180s 1ms/step - loss: 0.0406 - acc: 0.9842\n",
      "9 1 0.9891467313781334\n",
      "epoch 1 improved from 0.9851643766644852 to 0.9891467313781334\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 180s 1ms/step - loss: 0.0341 - acc: 0.9863\n",
      "9 2 0.988784301193906\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 180s 1ms/step - loss: 0.0288 - acc: 0.9884\n",
      "9 3 0.9875924337557224\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 180s 1ms/step - loss: 0.0237 - acc: 0.9906\n",
      "9 4 0.9862443262723991\n",
      "Epoch 1/1\n",
      "143718/143718 [==============================] - 180s 1ms/step - loss: 0.0194 - acc: 0.9924\n",
      "9 5 0.9844588931798319\n",
      "fold 10\n",
      "train_shape\n",
      "(143648, 150) (143648, 6)\n",
      "val_shape\n",
      "(15923, 150) (15923, 6)\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 185s 1ms/step - loss: 0.0676 - acc: 0.9773\n",
      "10 0 0.9817212014953723\n",
      "epoch 0 improved from -1 to 0.9817212014953723\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 181s 1ms/step - loss: 0.0419 - acc: 0.9837\n",
      "10 1 0.9863426564543092\n",
      "epoch 1 improved from 0.9817212014953723 to 0.9863426564543092\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 180s 1ms/step - loss: 0.0357 - acc: 0.9858\n",
      "10 2 0.9884405537693558\n",
      "epoch 2 improved from 0.9863426564543092 to 0.9884405537693558\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 180s 1ms/step - loss: 0.0302 - acc: 0.9879\n",
      "10 3 0.98796135334154\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 180s 1ms/step - loss: 0.0250 - acc: 0.9900\n",
      "10 4 0.9874945392353887\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 180s 1ms/step - loss: 0.0204 - acc: 0.9920\n",
      "10 5 0.987138600923712\n",
      "Epoch 1/1\n",
      "143648/143648 [==============================] - 180s 1ms/step - loss: 0.0167 - acc: 0.9937\n",
      "10 6 0.9853297257210211\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,Kfold+1):\n",
    "    \n",
    "    idx_train = train[train['cv_id'] != i].index\n",
    "    idx_val = train[train['cv_id'] == i].index\n",
    "    valid_id = train[train['cv_id'] == i]['id'].values\n",
    "    data_train = X_train_sequences_pad[idx_train]\n",
    "    labels_train = y_train_list[idx_train]\n",
    "    data_val = X_train_sequences_pad[idx_val]\n",
    "    labels_val = y_train_list[idx_val]\n",
    "    print(\"fold %d\"%(i))\n",
    "    print(\"train_shape\")\n",
    "    print(data_train.shape, labels_train.shape)\n",
    "    print(\"val_shape\")\n",
    "    print(data_val.shape, labels_val.shape)\n",
    "    model = get_model()\n",
    "    best = [-1, 0, 0, 0]\n",
    "    earlystop = 3\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.fit(data_train,labels_train,batch_size=256, epochs=1, verbose=1)\n",
    "        r = model.predict(data_val ,batch_size=256)\n",
    "        s = roc_auc_score(labels_val,r)\n",
    "        print(i,epoch,s)\n",
    "        if s > best[0]:\n",
    "            print(\"epoch \" + str(epoch) + \" improved from \" + str(best[0]) + \" to \" + str(s))\n",
    "            best = [s,epoch,copy.copy(model),r]\n",
    "        if epoch-best[1]>earlystop:\n",
    "            break\n",
    "    #save cv_results\n",
    "    tpd=pd.DataFrame(columns=[['id']+list_classes])\n",
    "    tpd['id'] = valid_id\n",
    "    tpd[list_classes] = best[-1]\n",
    "    cv_results.append(tpd)\n",
    "    cv_models.append(best[2])\n",
    "    cv_scores.append(best[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9843001310513645, 0.9861773692176916, 0.9863888724993432, 0.987653416089057, 0.9896646228479963, 0.9847420020607728, 0.9886027054089528, 0.9864531247771567, 0.9891467313781334, 0.9884405537693558] 0.9871569529099824\n",
      "prediction begin....\n"
     ]
    }
   ],
   "source": [
    "r=[]\n",
    "avg_val_score = np.average(cv_scores)\n",
    "print(cv_scores,avg_val_score)\n",
    "print(\"prediction begin....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction 0\n",
      "prediction 1\n",
      "prediction 2\n",
      "prediction 3\n",
      "prediction 4\n",
      "prediction 5\n",
      "prediction 6\n",
      "prediction 7\n",
      "prediction 8\n",
      "prediction 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(Kfold):\n",
    "    print(\"prediction \"+ str(i))\n",
    "    if len(r) == 0:\n",
    "        r = cv_models[i].predict(X_test_sequences_pad,batch_size=256)\n",
    "    else:\n",
    "        r += cv_models[i].predict(X_test_sequences_pad,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r /= 10\n",
    "index = 'bi-lstm-10-fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(cv_results).to_csv(\"%.4lstm_cv\"% (avg_val_score)+str(index)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm[list_classes] = r\n",
    "\n",
    "subm.to_csv(\"%.4lstm_submssion\"% (avg_val_score)+ index+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
