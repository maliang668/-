{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv1D, MaxPool2D,merge\n",
    "from keras.layers import Reshape, Flatten, concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('train_pre2.csv')\n",
    "test = pd.read_csv('test_pre2.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 200\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences_pad = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_sequences_pad = sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = model[word] if word in model else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.4f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "\n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "\n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [2,3,4,5]\n",
    "num_filters = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():    \n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    conv_0 = Conv1D(num_filters, filter_sizes[0], kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_1 = Conv1D(num_filters, filter_sizes[1], kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_2 = Conv1D(num_filters, filter_sizes[2], kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_3 = Conv1D(num_filters, filter_sizes[3], kernel_initializer='normal',activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3],axis=1)\n",
    "    \n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(64, activation='relu')(output)        \n",
    "    output = Dense(6, activation=\"sigmoid\")(output)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mskf = MultilabelStratifiedKFold(n_splits= 10, random_state=423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 300)     30000000    input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_9 (SpatialDro (None, 200, 300)     0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 199, 128)     76928       spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 198, 128)     115328      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 197, 128)     153728      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 196, 128)     192128      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_18 (KMaxPooling)  (None, 384)          0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_19 (KMaxPooling)  (None, 384)          0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_20 (KMaxPooling)  (None, 384)          0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_21 (KMaxPooling)  (None, 384)          0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1536)         0           k_max_pooling_18[0][0]           \n",
      "                                                                 k_max_pooling_19[0][0]           \n",
      "                                                                 k_max_pooling_20[0][0]           \n",
      "                                                                 k_max_pooling_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           98368       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            390         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,636,870\n",
      "Trainable params: 30,636,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 145s - loss: 0.0632 - acc: 0.9781 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 2/50\n",
      " - 141s - loss: 0.0426 - acc: 0.9835 - val_loss: 0.0422 - val_acc: 0.9834\n",
      "Epoch 3/50\n",
      " - 141s - loss: 0.0373 - acc: 0.9851 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 4/50\n",
      " - 141s - loss: 0.0327 - acc: 0.9867 - val_loss: 0.0428 - val_acc: 0.9832\n",
      "Epoch 5/50\n",
      " - 141s - loss: 0.0289 - acc: 0.9882 - val_loss: 0.0464 - val_acc: 0.9834\n",
      "Epoch 6/50\n",
      " - 141s - loss: 0.0256 - acc: 0.9895 - val_loss: 0.0498 - val_acc: 0.9828\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 141s - loss: 0.0263 - acc: 0.9898 - val_loss: 0.0197 - val_acc: 0.9924\n",
      "Epoch 2/50\n",
      " - 141s - loss: 0.0220 - acc: 0.9913 - val_loss: 0.0210 - val_acc: 0.9917\n",
      "Epoch 3/50\n",
      " - 141s - loss: 0.0191 - acc: 0.9925 - val_loss: 0.0229 - val_acc: 0.9909\n",
      "Epoch 4/50\n",
      " - 141s - loss: 0.0168 - acc: 0.9935 - val_loss: 0.0250 - val_acc: 0.9901\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 141s - loss: 0.0173 - acc: 0.9932 - val_loss: 0.0119 - val_acc: 0.9957\n",
      "Epoch 2/50\n",
      " - 141s - loss: 0.0150 - acc: 0.9942 - val_loss: 0.0119 - val_acc: 0.9952\n",
      "Epoch 3/50\n",
      " - 141s - loss: 0.0132 - acc: 0.9949 - val_loss: 0.0137 - val_acc: 0.9946\n",
      "Epoch 4/50\n",
      " - 141s - loss: 0.0120 - acc: 0.9954 - val_loss: 0.0152 - val_acc: 0.9940\n",
      "Epoch 5/50\n",
      " - 141s - loss: 0.0111 - acc: 0.9958 - val_loss: 0.0167 - val_acc: 0.9936\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0118 - acc: 0.9955 - val_loss: 0.0070 - val_acc: 0.9976\n",
      "Epoch 2/50\n",
      " - 141s - loss: 0.0100 - acc: 0.9962 - val_loss: 0.0081 - val_acc: 0.9972\n",
      "Epoch 3/50\n",
      " - 141s - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0092 - val_acc: 0.9968\n",
      "Epoch 4/50\n",
      " - 141s - loss: 0.0088 - acc: 0.9967 - val_loss: 0.0103 - val_acc: 0.9962\n",
      "train_nums, 143613 test_nums: 15958\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/50\n",
      " - 141s - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0051 - val_acc: 0.9983\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0059 - val_acc: 0.9978\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0077 - acc: 0.9971 - val_loss: 0.0072 - val_acc: 0.9973\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0072 - acc: 0.9973 - val_loss: 0.0078 - val_acc: 0.9972\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0079 - acc: 0.9971 - val_loss: 0.0041 - val_acc: 0.9985\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0043 - val_acc: 0.9984\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0067 - acc: 0.9974 - val_loss: 0.0053 - val_acc: 0.9980\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0063 - acc: 0.9976 - val_loss: 0.0057 - val_acc: 0.9979\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 0.9989\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0036 - val_acc: 0.9988\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0058 - acc: 0.9978 - val_loss: 0.0048 - val_acc: 0.9984\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0054 - acc: 0.9979 - val_loss: 0.0051 - val_acc: 0.9983\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0028 - val_acc: 0.9990\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0052 - acc: 0.9980 - val_loss: 0.0037 - val_acc: 0.9986\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0040 - val_acc: 0.9985\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0045 - val_acc: 0.9984\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0026 - val_acc: 0.9991\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0047 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 0.9989\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0046 - acc: 0.9983 - val_loss: 0.0035 - val_acc: 0.9988\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0045 - val_acc: 0.9984\n",
      "train_nums, 143614 test_nums: 15957\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      " - 140s - loss: 0.0047 - acc: 0.9983 - val_loss: 0.0021 - val_acc: 0.9992\n",
      "Epoch 2/50\n",
      " - 140s - loss: 0.0045 - acc: 0.9983 - val_loss: 0.0024 - val_acc: 0.9991\n",
      "Epoch 3/50\n",
      " - 140s - loss: 0.0041 - acc: 0.9985 - val_loss: 0.0029 - val_acc: 0.9989\n",
      "Epoch 4/50\n",
      " - 140s - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0033 - val_acc: 0.9988\n"
     ]
    }
   ],
   "source": [
    "subs = []\n",
    "for train_index,text_index in mskf.split(X_train_sequences_pad,y_train):        \n",
    "    print('train_nums,',len(train_index),'test_nums:',len(text_index))\n",
    "    train_x,text_x = X_train_sequences_pad[train_index],X_train_sequences_pad[text_index]\n",
    "    train_y,text_y = y_train[train_index],y_train[text_index]\n",
    "    hist = model.fit(train_x,train_y,batch_size = 256,epochs = 50,validation_data = (text_x,text_y), verbose = 2,callbacks = [early])\n",
    "        \n",
    "    #result = model1.predict(text_x,batch_size = 256)\n",
    "    #s = roc_auc_score(text_y,result)\n",
    "    #print('roc_auc_score:',s)\n",
    "        \n",
    "    '''tpd = pd.DataFrame(columns=[['id']+list_classes])\n",
    "    tpd[list_classes] = result\n",
    "    tpd ['id'] = text_index\n",
    "    text_cnn_result.append(tpd)'''\n",
    "        \n",
    "    sub = model.predict(X_test_sequences_pad,batch_size = 256)\n",
    "        \n",
    "    subs.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cv = sum(subs)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[list_classes] = sub_cv\n",
    "submission.to_csv(\"kmax_cnn_sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
