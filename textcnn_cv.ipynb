{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('train_pre2.csv')\n",
    "test = pd.read_csv('test_pre2.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cv_id'] = [random.randint(1,10) for _ in range(len(train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cv_id'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train_list = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test_list = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "max_len = 200\n",
    "embed_size = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train_list) + list(X_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_list)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences_pad = sequence.pad_sequences(X_train_sequences, maxlen=max_len)\n",
    "X_test_sequences_pad = sequence.pad_sequences(X_test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(max_features, len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = model[word] if word in model else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_fasttext = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_glove = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196016\n"
     ]
    }
   ],
   "source": [
    "embedding_index_glove = {}\n",
    "with open(embedding_glove,encoding ='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        cof = np.asarray(values[1:],dtype = 'float32')\n",
    "        embedding_index_glove[word] = cof\n",
    "print(len(embedding_index_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_glove = np.zeros((nb_words,embed_size))\n",
    "for word,i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vec_glove = embedding_index_glove.get(word)\n",
    "    if embedding_vec_glove is not None:\n",
    "        embedding_matrix_glove[i] = embedding_vec_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [3,4,5]\n",
    "num_filters =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():    \n",
    "    inp = Input(shape=(max_len, ))\n",
    "    #fasttext\n",
    "    fast_text_embedding = Embedding(max_features, embed_size, weights=[embedding_matrix_fasttext])(inp)\n",
    "    x = SpatialDropout1D(0.1)(fast_text_embedding)\n",
    "    x = Reshape((max_len, embed_size, 1))(x)\n",
    "    \n",
    "\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1),strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1),strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1),strides=(1,1), padding='valid')(conv_2)\n",
    "        \n",
    "    concatenated_tensor1  = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])   \n",
    "\n",
    "    \n",
    "    \n",
    "    #glove\n",
    "    glove_embedding = Embedding(max_features, embed_size, weights=[embedding_matrix_glove])(inp)\n",
    "    y = SpatialDropout1D(0.1)(glove_embedding)\n",
    "    y = Reshape((max_len, embed_size, 1))(y)\n",
    "    \n",
    "\n",
    "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',activation='elu')(x)\n",
    "    \n",
    "    maxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1),strides=(1,1), padding='valid')(conv_3)\n",
    "    maxpool_4 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1),strides=(1,1), padding='valid')(conv_4)\n",
    "    maxpool_5 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1),strides=(1,1), padding='valid')(conv_5)\n",
    "        \n",
    "    concatenated_tensor2  = Concatenate(axis=1)([maxpool_3, maxpool_4, maxpool_5])   \n",
    "\n",
    "    \n",
    "    #\n",
    "    concatenated_tensor = Concatenate([concatenated_tensor1,concatenated_tensor2])\n",
    "    flatten = Flatten()(concatenated_tensor2)\n",
    "    dropout = Dropout(0.1)(flatten)\n",
    "        \n",
    "    output = Dense(6, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models=[]\n",
    "cv_results=[]\n",
    "cv_scores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kfold = 10\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143512, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train = train[train['cv_id'] != 1].index\n",
    "data_train = X_train_sequences_pad[idx_train]\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "train_shape\n",
      "(143512, 200) (143512, 6)\n",
      "val_shape\n",
      "(16059, 200) (16059, 6)\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 90s 626us/step - loss: 0.0607 - acc: 0.9792\n",
      "1 0 0.9841871640524714\n",
      "epoch 0 improved from -1 to 0.9841871640524714\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 87s 606us/step - loss: 0.0384 - acc: 0.9851\n",
      "1 1 0.9868000516562327\n",
      "epoch 1 improved from 0.9841871640524714 to 0.9868000516562327\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 87s 606us/step - loss: 0.0302 - acc: 0.9882\n",
      "1 2 0.98601408978588\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 87s 607us/step - loss: 0.0227 - acc: 0.9913\n",
      "1 3 0.984810757080663\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 87s 606us/step - loss: 0.0163 - acc: 0.9942\n",
      "1 4 0.9831064138932907\n",
      "Epoch 1/1\n",
      "143512/143512 [==============================] - 87s 606us/step - loss: 0.0120 - acc: 0.9959\n",
      "1 5 0.9813887012442492\n",
      "fold 2\n",
      "train_shape\n",
      "(143637, 200) (143637, 6)\n",
      "val_shape\n",
      "(15934, 200) (15934, 6)\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 89s 621us/step - loss: 0.0593 - acc: 0.9802\n",
      "2 0 0.9854065169519162\n",
      "epoch 0 improved from -1 to 0.9854065169519162\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 87s 606us/step - loss: 0.0387 - acc: 0.9851\n",
      "2 1 0.9876680432947991\n",
      "epoch 1 improved from 0.9854065169519162 to 0.9876680432947991\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 87s 607us/step - loss: 0.0304 - acc: 0.9883\n",
      "2 2 0.9873545635845247\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 87s 607us/step - loss: 0.0227 - acc: 0.9913\n",
      "2 3 0.9867533966206001\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 87s 607us/step - loss: 0.0164 - acc: 0.9941\n",
      "2 4 0.9855158241511793\n",
      "Epoch 1/1\n",
      "143637/143637 [==============================] - 87s 606us/step - loss: 0.0121 - acc: 0.9959\n",
      "2 5 0.9841354108421826\n",
      "fold 3\n",
      "train_shape\n",
      "(143677, 200) (143677, 6)\n",
      "val_shape\n",
      "(15894, 200) (15894, 6)\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 89s 621us/step - loss: 0.0621 - acc: 0.9784\n",
      "3 0 0.98793734706529\n",
      "epoch 0 improved from -1 to 0.98793734706529\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 87s 607us/step - loss: 0.0390 - acc: 0.9850\n",
      "3 1 0.9893580013575226\n",
      "epoch 1 improved from 0.98793734706529 to 0.9893580013575226\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 87s 607us/step - loss: 0.0310 - acc: 0.9880\n",
      "3 2 0.9890759512256472\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 87s 607us/step - loss: 0.0237 - acc: 0.9908\n",
      "3 3 0.9883202741480797\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 87s 606us/step - loss: 0.0172 - acc: 0.9938\n",
      "3 4 0.986564141044283\n",
      "Epoch 1/1\n",
      "143677/143677 [==============================] - 87s 607us/step - loss: 0.0125 - acc: 0.9958\n",
      "3 5 0.9850307775258825\n",
      "fold 4\n",
      "train_shape\n",
      "(143739, 200) (143739, 6)\n",
      "val_shape\n",
      "(15832, 200) (15832, 6)\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 89s 621us/step - loss: 0.0674 - acc: 0.9764\n",
      "4 0 0.9874550947598624\n",
      "epoch 0 improved from -1 to 0.9874550947598624\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 607us/step - loss: 0.0396 - acc: 0.9849\n",
      "4 1 0.9888894101087305\n",
      "epoch 1 improved from 0.9874550947598624 to 0.9888894101087305\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 607us/step - loss: 0.0314 - acc: 0.9878\n",
      "4 2 0.9889511199044531\n",
      "epoch 2 improved from 0.9888894101087305 to 0.9889511199044531\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 606us/step - loss: 0.0241 - acc: 0.9908\n",
      "4 3 0.9875379269584136\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 607us/step - loss: 0.0177 - acc: 0.9935\n",
      "4 4 0.9865303854508974\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 607us/step - loss: 0.0130 - acc: 0.9955\n",
      "4 5 0.9851773850249222\n",
      "Epoch 1/1\n",
      "143739/143739 [==============================] - 87s 606us/step - loss: 0.0097 - acc: 0.9969\n",
      "4 6 0.9823459053905276\n",
      "fold 5\n",
      "train_shape\n",
      "(143682, 200) (143682, 6)\n",
      "val_shape\n",
      "(15889, 200) (15889, 6)\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 89s 622us/step - loss: 0.0655 - acc: 0.9766\n",
      "5 0 0.9857410333965863\n",
      "epoch 0 improved from -1 to 0.9857410333965863\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0392 - acc: 0.9850\n",
      "5 1 0.9873503800601494\n",
      "epoch 1 improved from 0.9857410333965863 to 0.9873503800601494\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0311 - acc: 0.9880\n",
      "5 2 0.9873759209042451\n",
      "epoch 2 improved from 0.9873503800601494 to 0.9873759209042451\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0235 - acc: 0.9910\n",
      "5 3 0.9856204386936486\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0174 - acc: 0.9937\n",
      "5 4 0.984326901076916\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0127 - acc: 0.9957\n",
      "5 5 0.9824098644205574\n",
      "Epoch 1/1\n",
      "143682/143682 [==============================] - 87s 607us/step - loss: 0.0098 - acc: 0.9969\n",
      "5 6 0.9809009468723113\n",
      "fold 6\n",
      "train_shape\n",
      "(143456, 200) (143456, 6)\n",
      "val_shape\n",
      "(16115, 200) (16115, 6)\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 89s 622us/step - loss: 0.0693 - acc: 0.9750\n",
      "6 0 0.9852109741797724\n",
      "epoch 0 improved from -1 to 0.9852109741797724\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 87s 607us/step - loss: 0.0397 - acc: 0.9847\n",
      "6 1 0.9884787237197993\n",
      "epoch 1 improved from 0.9852109741797724 to 0.9884787237197993\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 87s 607us/step - loss: 0.0316 - acc: 0.9878\n",
      "6 2 0.9882817997206658\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 87s 607us/step - loss: 0.0242 - acc: 0.9907\n",
      "6 3 0.9877314827036208\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 87s 607us/step - loss: 0.0179 - acc: 0.9935\n",
      "6 4 0.9860287938725588\n",
      "Epoch 1/1\n",
      "143456/143456 [==============================] - 87s 607us/step - loss: 0.0130 - acc: 0.9956\n",
      "6 5 0.9854098098550136\n",
      "fold 7\n",
      "train_shape\n",
      "(143619, 200) (143619, 6)\n",
      "val_shape\n",
      "(15952, 200) (15952, 6)\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 89s 622us/step - loss: 0.0625 - acc: 0.9780\n",
      "7 0 0.9862920143088054\n",
      "epoch 0 improved from -1 to 0.9862920143088054\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 87s 608us/step - loss: 0.0390 - acc: 0.9850\n",
      "7 1 0.9886640903206971\n",
      "epoch 1 improved from 0.9862920143088054 to 0.9886640903206971\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 87s 608us/step - loss: 0.0311 - acc: 0.9878\n",
      "7 2 0.9881682312900595\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 87s 608us/step - loss: 0.0236 - acc: 0.9909\n",
      "7 3 0.987280959865441\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 87s 607us/step - loss: 0.0172 - acc: 0.9937\n",
      "7 4 0.9856369895449174\n",
      "Epoch 1/1\n",
      "143619/143619 [==============================] - 87s 607us/step - loss: 0.0127 - acc: 0.9957\n",
      "7 5 0.9845080081355855\n",
      "fold 8\n",
      "train_shape\n",
      "(143685, 200) (143685, 6)\n",
      "val_shape\n",
      "(15886, 200) (15886, 6)\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 90s 625us/step - loss: 0.0624 - acc: 0.9787\n",
      "8 0 0.9841950922607151\n",
      "epoch 0 improved from -1 to 0.9841950922607151\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 88s 611us/step - loss: 0.0391 - acc: 0.9850\n",
      "8 1 0.986410475752124\n",
      "epoch 1 improved from 0.9841950922607151 to 0.986410475752124\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 88s 610us/step - loss: 0.0309 - acc: 0.9881\n",
      "8 2 0.9861294750367078\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 88s 610us/step - loss: 0.0235 - acc: 0.9910\n",
      "8 3 0.9843512099297289\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 88s 610us/step - loss: 0.0171 - acc: 0.9939\n",
      "8 4 0.9828896270794658\n",
      "Epoch 1/1\n",
      "143685/143685 [==============================] - 88s 611us/step - loss: 0.0125 - acc: 0.9957\n",
      "8 5 0.9831127309102238\n",
      "fold 9\n",
      "train_shape\n",
      "(143643, 200) (143643, 6)\n",
      "val_shape\n",
      "(15928, 200) (15928, 6)\n",
      "Epoch 1/1\n",
      "143643/143643 [==============================] - 90s 628us/step - loss: 0.0614 - acc: 0.9788\n",
      "9 0 0.9848435398872649\n",
      "epoch 0 improved from -1 to 0.9848435398872649\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143643/143643 [==============================] - 88s 611us/step - loss: 0.0386 - acc: 0.9851\n",
      "9 1 0.9864503151044315\n",
      "epoch 1 improved from 0.9848435398872649 to 0.9864503151044315\n",
      "Epoch 1/1\n",
      "143643/143643 [==============================] - 88s 611us/step - loss: 0.0304 - acc: 0.9882\n",
      "9 2 0.9861379280327207\n",
      "Epoch 1/1\n",
      "143643/143643 [==============================] - 88s 611us/step - loss: 0.0234 - acc: 0.9910\n",
      "9 3 0.9851777456434953\n",
      "Epoch 1/1\n",
      "143643/143643 [==============================] - 88s 611us/step - loss: 0.0171 - acc: 0.9939\n",
      "9 4 0.9833522075081591\n",
      "Epoch 1/1\n",
      "143643/143643 [==============================] - 88s 611us/step - loss: 0.0124 - acc: 0.9957\n",
      "9 5 0.981676716624043\n",
      "fold 10\n",
      "train_shape\n",
      "(143489, 200) (143489, 6)\n",
      "val_shape\n",
      "(16082, 200) (16082, 6)\n",
      "Epoch 1/1\n",
      "143489/143489 [==============================] - 90s 629us/step - loss: 0.0672 - acc: 0.9759\n",
      "10 0 0.9783977962383074\n",
      "epoch 0 improved from -1 to 0.9783977962383074\n",
      "Epoch 1/1\n",
      "143489/143489 [==============================] - 88s 610us/step - loss: 0.0390 - acc: 0.9850\n",
      "10 1 0.9783471275310512\n",
      "Epoch 1/1\n",
      "143489/143489 [==============================] - 87s 609us/step - loss: 0.0311 - acc: 0.9879\n",
      "10 2 0.9780853694506488\n",
      "Epoch 1/1\n",
      "143489/143489 [==============================] - 87s 609us/step - loss: 0.0238 - acc: 0.9909\n",
      "10 3 0.9768960037122637\n",
      "Epoch 1/1\n",
      "143489/143489 [==============================] - 87s 609us/step - loss: 0.0175 - acc: 0.9936\n",
      "10 4 0.975235550608324\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,Kfold+1):\n",
    "    \n",
    "    idx_train = train[train['cv_id'] != i].index\n",
    "    idx_val = train[train['cv_id'] == i].index\n",
    "    valid_id = train[train['cv_id'] == i]['id'].values\n",
    "    data_train = X_train_sequences_pad[idx_train]\n",
    "    labels_train = y_train_list[idx_train]\n",
    "    data_val = X_train_sequences_pad[idx_val]\n",
    "    labels_val = y_train_list[idx_val]\n",
    "    print(\"fold %d\"%(i))\n",
    "    print(\"train_shape\")\n",
    "    print(data_train.shape, labels_train.shape)\n",
    "    print(\"val_shape\")\n",
    "    print(data_val.shape, labels_val.shape)\n",
    "    model = get_model()\n",
    "    best = [-1, 0, 0, 0]\n",
    "    earlystop = 3\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        model.fit(data_train,labels_train,batch_size=256, epochs=1, verbose=1)\n",
    "        r = model.predict(data_val ,batch_size=256)\n",
    "        s = roc_auc_score(labels_val,r)\n",
    "        print(i,epoch,s)\n",
    "        if s > best[0]:\n",
    "            print(\"epoch \" + str(epoch) + \" improved from \" + str(best[0]) + \" to \" + str(s))\n",
    "            best = [s,epoch,copy.copy(model),r]\n",
    "        if epoch-best[1]>earlystop:\n",
    "            break\n",
    "    #save cv_results\n",
    "    tpd=pd.DataFrame(columns=[['id']+list_classes])\n",
    "    tpd['id'] = valid_id\n",
    "    tpd[list_classes] = best[-1]\n",
    "    cv_results.append(tpd)\n",
    "    cv_models.append(best[2])\n",
    "    cv_scores.append(best[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9868000516562327, 0.9876680432947991, 0.9893580013575226, 0.9889511199044531, 0.9873759209042451, 0.9884787237197993, 0.9886640903206971, 0.986410475752124, 0.9864503151044315, 0.9783977962383074] 0.9868554538252612\n",
      "prediction begin....\n"
     ]
    }
   ],
   "source": [
    "r=[]\n",
    "avg_val_score = np.average(cv_scores)\n",
    "print(cv_scores,avg_val_score)\n",
    "print(\"prediction begin....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction 0\n",
      "prediction 1\n",
      "prediction 2\n",
      "prediction 3\n",
      "prediction 4\n",
      "prediction 5\n",
      "prediction 6\n",
      "prediction 7\n",
      "prediction 8\n",
      "prediction 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(Kfold):\n",
    "    print(\"prediction \"+ str(i))\n",
    "    if len(r) == 0:\n",
    "        r = cv_models[i].predict(X_test_sequences_pad,batch_size=256)\n",
    "    else:\n",
    "        r += cv_models[i].predict(X_test_sequences_pad,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "r /= 10\n",
    "index = 'cnn-10-fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(cv_results).to_csv(\"%.4ftextcnn_cv_\"% (avg_val_score)+str(index)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[list_classes] = r\n",
    "\n",
    "sample_submission.to_csv(\"%.4ftextcnn_submssion\"% (avg_val_score)+ index+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
